{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a67090-c08b-45fa-95b5-020487b60004",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53275b0a-f9e1-49fd-bda0-e6a2f1b80026",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from joblib import dump, load\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import BaggingRegressor as BR, RandomForestRegressor as RFR\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, ParameterGrid\n",
    "\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "TRAIN_SMALL_PATH = \"./etc/training_small.csv\"\n",
    "TRAIN_LARGE_PATH = \"./etc/training_large.csv\"\n",
    "TEST_PATH = \"./etc/test.csv\"\n",
    "TARGET = \"price\"\n",
    "\n",
    "HIGH_RELEVANCE_COLS = ['price', 'city_fuel_economy', 'engine_displacement', 'horsepower', 'highway_fuel_economy', 'year', 'mileage', 'owner_count', 'make_name', 'body_type']\n",
    "NUMERICAL_COLS = [\"price\", 'city_fuel_economy', 'engine_displacement', 'horsepower', 'highway_fuel_economy', 'year', \"mileage\", \"owner_count\"]\n",
    "CATEGORICAL_COLS = [\"make_name\", 'body_type'] #, 'frame_damaged' \"model_name\" ]\n",
    "\n",
    "BAGGING_PARAMS = {\n",
    "    'n_estimators': np.arange(10, 101, 10),  # Number of base estimators in the ensemble\n",
    "    'max_samples': np.arange(0.1, 1.1, 0.1),  # The number of samples to draw from X to train each base estimator\n",
    "    'max_features': np.arange(0.1, 1.1, 0.1),  # The number of features to draw from X to train each base estimator\n",
    "    'bootstrap': [True, False],  # Whether samples are drawn with replacement\n",
    "    # 'bootstrap_features': [True, False],  # Whether features are drawn with replacement\n",
    "    'oob_score': [True, False],  # Whether to use out-of-bag samples to estimate the generalization error\n",
    "}\n",
    "\n",
    "RFR_PARAMS = {\n",
    "    'n_estimators': np.arange(10, 101, 10),  # Number of trees in the forest\n",
    "    'max_depth': np.arange(3, 17, 2).tolist() + [None],  # Maximum depth of the tree\n",
    "    'min_samples_split': np.arange(2, 11, 2),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': np.arange(1, 11, 2),  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['sqrt', 'log2', 1.0],  # Number of features to consider when looking for the best split\n",
    "    # 'bootstrap': [True, False],  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "XGBR_PARAMS = {\n",
    "    'n_estimators': np.arange(10, 101, 10),  # Number of gradient boosted trees\n",
    "    'learning_rate': [0.001, 0.01, 0.05],  # Step size shrinkage to prevent overfitting\n",
    "    'max_depth': np.arange(3, 11, 2),  # Maximum depth of a tree\n",
    "    # 'min_child_weight': np.arange(1, 6, 1),  # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'gamma': [i/10.0 for i in range(0, 5)],  # Minimum loss reduction required to make a further partition\n",
    "    'subsample': [i/10.0 for i in range(6, 11)],  # Fraction of samples used per tree\n",
    "    # 'colsample_bytree': [i/10.0 for i in range(6, 11)],  # Fraction of features used per tree\n",
    "    # 'scale_pos_weight': [1, 10, 25, 50, 75, 99, 100],  # Control the balance of positive and negative weights\n",
    "    'reg_alpha': [1e-2, 0.1, 1, 100],  # L1 regularization term on weights\n",
    "    # 'reg_lambda': [1e-2, 0.1, 1, 100],  # L2 regularization term on weights\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc7b41-e2b6-492b-8729-67c58cfa415c",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a992cb10-6342-47bc-9526-88667fab7f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(dataFrame, cols) -> pd.DataFrame:\n",
    "    les = {}\n",
    "    for i, col in enumerate(cols):\n",
    "        le = LabelEncoder()\n",
    "        dataFrame[col] = le.fit_transform(dataFrame[col])\n",
    "        les[col] = le\n",
    "    return les\n",
    "\n",
    "def preprocess(df: pd.DataFrame, testing: bool = False, les = None):\n",
    "    # Create a copy of the DataFrame\n",
    "    X = df.copy()\n",
    "\n",
    "    # Select only a few columns\n",
    "    X = X[HIGH_RELEVANCE_COLS]\n",
    "    \n",
    "    if (testing):\n",
    "        y = None\n",
    "    else:\n",
    "        # Get the target\n",
    "        y = X[\"price\"].apply(lambda x: math.log(x))\n",
    "\n",
    "    # Drop the \"price\" column because we don't need it anymore\n",
    "    X.drop(\"price\", axis=1, inplace=True)\n",
    "    \n",
    "    # Feature engineering part 1\n",
    "    # Group 'owner_count' >= 4 into 4\n",
    "    X['owner_count'] = X['owner_count'].apply(lambda x: 4 if x >= 4 else x)\n",
    "    \n",
    "    # Transformers: I use different transformers for numerical and categorical columns.\n",
    "    numerical_transformer = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('imputer', SimpleImputer(strategy='mean'))\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median'))\n",
    "    ])\n",
    "    \n",
    "    les = label_encode(X, CATEGORICAL_COLS)\n",
    "    \n",
    "    # Define a simplified ColumnTransformer with a single transformer\n",
    "    # The steps of this pipeline are as follows:\n",
    "    # 1. Scaling: Scales values according to the N(0, 1) distribution\n",
    "    # 2. Impute: Deals with NaNs by imputing them with the mean\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, NUMERICAL_COLS[1:]), # Don't include \"price\"\n",
    "            ('cat', categorical_transformer, CATEGORICAL_COLS)\n",
    "        ])\n",
    "    \n",
    "    # Create a pipeline\n",
    "    pipe = Pipeline([('preprocessor', preprocessor)])\n",
    "    \n",
    "     # And fit it\n",
    "    X = pd.DataFrame(pipe.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Feature engineering part 2\n",
    "    X['avg_economy'] = (X['city_fuel_economy'] + X['highway_fuel_economy']) / 2    \n",
    "    X['engine_vol_vs_hp'] = X['engine_displacement'] / X['horsepower']\n",
    "    \n",
    "    # Deal with infinities\n",
    "    X['engine_vol_vs_hp'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    # Drop columns\n",
    "    X.drop(['city_fuel_economy', 'highway_fuel_economy', 'engine_displacement', 'horsepower'], axis=1, inplace=True)\n",
    "    \n",
    "    if (not testing):\n",
    "        # Convert the labels to integers\n",
    "        for col in CATEGORICAL_COLS:\n",
    "                X[col] = X[col].apply(lambda x: int(x))\n",
    "\n",
    "    return X, y, les\n",
    "\n",
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, params: dict):    \n",
    "    # Use GridSearchCV with the maximum number of CPU cores available\n",
    "    grid_search = GridSearchCV(model, params, cv=3, scoring=\"neg_mean_squared_error\", n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Starting grid search: \", grid_search)\n",
    "    \n",
    "    # Train the model using GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Grid search complete.\")\n",
    "\n",
    "    # Best estimator after grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict using the best model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    r_squared = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # Get cross-validation scores from GridSearchCV results\n",
    "    cv_scores = grid_search.cv_results_['mean_test_score']\n",
    "    \n",
    "    return {\n",
    "        'cv_scores': cv_scores,\n",
    "        'r_squared': r_squared,\n",
    "        'mse': mse,\n",
    "        'y_pred': y_pred,\n",
    "        'model': best_model\n",
    "    }\n",
    "\n",
    "def plot_residuals(actual_values, predicted_values, filename: str):\n",
    "    residuals = actual_values - predicted_values\n",
    "    \n",
    "    skew = stats.skew(residuals)\n",
    "    kurtosis = stats.kurtosis(residuals)\n",
    "    print(\"Skew: \", skew)\n",
    "    print(\"Kurtosis: \", kurtosis)\n",
    "    \n",
    "    # Set up the subplot layout\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Residuals Plot\n",
    "    ax[0].scatter(predicted_values, residuals, alpha=0.6)\n",
    "    ax[0].axhline(y=0, color='r', linestyle='--')\n",
    "    ax[0].set_xlabel('Predicted Values')\n",
    "    ax[0].set_ylabel('Residuals')\n",
    "    ax[0].set_title('Residuals vs. Predicted Values')\n",
    "\n",
    "    # Q-Q plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=ax[1])\n",
    "    ax[1].set_title('Q-Q plot of the Residuals')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./img/{filename}_residuals.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return skew, kurtosis\n",
    "\n",
    "def plot_actual_vs_predicted(actual_values, predicted_values, filename: str, title: str ='Predicted vs Actual'):\n",
    "    \"\"\"\n",
    "    Plot actual values against predicted values in a scatter plot.\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_values: Actual target values.\n",
    "    - predicted_values: Predicted values from the model.\n",
    "    - title: Title of the plot.\n",
    "    \n",
    "    Returns:\n",
    "    - A scatter plot of actual vs. predicted values.\n",
    "    \"\"\"    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(actual_values, predicted_values, alpha=0.5)\n",
    "    plt.plot([min(actual_values), max(actual_values)], [min(actual_values), max(actual_values)], color='red')  # Diagonal line\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"./img/{filename}.png\")\n",
    "    plt.show()\n",
    "\n",
    "def get_test_preds(model, les, model_name, large: bool = False):\n",
    "    # Load test data\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    \n",
    "    # Preprocess test data\n",
    "    test_preprocessed, _, _ = preprocess(test, True, les)\n",
    "    \n",
    "    print(test_preprocessed.head())\n",
    "    \n",
    "    # Get predictions\n",
    "    print(\"Predictions:\")\n",
    "    preds = model.predict(test_preprocessed)\n",
    "    df = pd.DataFrame(preds)\n",
    "    print(df.head())\n",
    "\n",
    "    # And save them\n",
    "    filename = f\"./etc/test_preds_{model_name}.csv\"\n",
    "    np.savetxt(filename, preds, delimiter=\",\", header=TARGET)\n",
    "\n",
    "    # Sanity check\n",
    "    d = pd.read_csv(filename)\n",
    "    print(d.shape)\n",
    "    print(d.head())\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def master(model_name: str, large: bool = False):\n",
    "    '''Master method to do everything.'''\n",
    "    \n",
    "    # Select the model and the parameters\n",
    "    model = None\n",
    "    gcv_params = None\n",
    "    if (model_name == \"BR\"):\n",
    "        model = BR\n",
    "        gcv_params = BAGGING_PARAMS\n",
    "    elif (model_name == \"RFR\"):\n",
    "        model = RFR\n",
    "        gcv_params = RFR_PARAMS\n",
    "    elif (model_name == \"XGBR\"):\n",
    "        model = XGBR\n",
    "        gcv_params = XGBR_PARAMS\n",
    "        \n",
    "    if (model is None or gcv_params is None):\n",
    "        raise Exception(\"Incorrect parameters in `master()`.\")\n",
    "    \n",
    "    # Select the dataset\n",
    "    dataset = TRAIN_SMALL_PATH\n",
    "    d = \"small\"\n",
    "    if (large):\n",
    "        dataset = TRAIN_LARGE_PATH\n",
    "        d = \"large\"\n",
    "    \n",
    "    print(\"Dataset: \", dataset)\n",
    "    train = pd.read_csv(dataset)\n",
    "    \n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "    # Preprocess the dataset\n",
    "    X, y, les = preprocess(train)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1203)\n",
    "    print(\"X_train.tail(2):\")\n",
    "    print(X_train.tail(2))\n",
    "    \n",
    "    print(\"------------------------------------\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    results = train_and_evaluate(model(), X_train, y_train, X_test, y_test, gcv_params)\n",
    "\n",
    "    # Extract the results\n",
    "    cv_scores = results['cv_scores']\n",
    "    r_squared = results['r_squared']\n",
    "    mse = results['mse']\n",
    "    best_model = results['model']\n",
    "    \n",
    "    name = f\"{model_name}_{d}\"\n",
    "\n",
    "    # Print or use the extracted results as needed\n",
    "    # print(f'Cross-Validation Scores: {cv_scores}')\n",
    "    print(f'R squared: {r_squared}')\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'Best Model: {best_model}')\n",
    "    \n",
    "    print(\"------------------------------------\")\n",
    "    print(\"Plotting...\")\n",
    "    y_preds = best_model.predict(X_test)\n",
    "    plot_actual_vs_predicted(y_test, y_preds, name, \"Predicted vs Actual\")\n",
    "    skew, kurtosis = plot_residuals(y_test, y_preds, f\"{name}_residuals\")\n",
    "    \n",
    "    results['skew'] = skew\n",
    "    results['kurtosis'] = kurtosis\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"Saving model...\")\n",
    "    dump(best_model, filename=name)\n",
    "    print(\"Saved.\")\n",
    "    \n",
    "    \n",
    "    print(\"------------------------------------\")\n",
    "    print(\"Testing...\")\n",
    "    get_test_preds(best_model, les, name, large)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da72a3c-bfde-4c04-b395-05e76be047ea",
   "metadata": {},
   "source": [
    "# Using the small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b28387-da73-4541-b9f4-8577128afc66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876b65a-d58c-444e-b3c0-546fb5b756fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "br_results_small = master(\"BR\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7853813a-f981-40f0-b72c-7883059673f6",
   "metadata": {},
   "source": [
    "## Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae3515-0dd2-4a87-88f5-638ae8ecdafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_results_small = master(\"RFR\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfa89f-1d5b-4867-8e5f-3cc112f49be0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (Extreme Gradient) Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b9b6d-23f9-4196-91cb-a66ddb611150",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr_results_small = master(\"XGBR\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842699e-983c-4b7c-bd81-964329ffd00b",
   "metadata": {},
   "source": [
    "# Using the large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b7032-02e3-4ada-8a59-b131860248de",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac1fba-0422-4b19-aad7-b9269aac696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "br_results_large = master(\"BR\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda60932-acac-48d5-bb00-0fccf3587c7f",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd764904-0d55-4ef6-ac64-4ab0a5507b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_results_large = master(\"RFR\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425b7a4-338c-4177-bbd0-156eebf2a002",
   "metadata": {},
   "source": [
    "## (Extreme Gradient) Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da651f55-ae91-40c0-9e94-599cc55b4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr_results_large = master(\"XGBR\", True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 user modules",
   "language": "python",
   "name": "python3-user-modules"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
